from utils.emebdding_utils import *
from utils.qdrant_helpers import *
from utils.llm import *
from utils.emebdding_utils import *


def rag_pipeline(embedding_model:SentenceTransformer, client:QdrantClient, llm, topic:str, query:str, limit:int=5):
    """
    Run the RAG pipeline.

    Args:
        embedding_model (SentenceTransformer): The embedding model.
        client (QdrantClient): The Qdrant client.
        llm: The language model.
        query (str): The query to search for.
        limit (int): The number of results to return.

    Returns:
        str: The answer generated by the language model.
    """
    
    # check if the topic is already in Qdrant
    exists= topic_exists(client, "wiki_chunks", topic)
    if not exists:
        print(f"Topic '{topic}' not found in Qdrant. Embedding and upserting...")
        
        # embed wiki content
        chunks, embeddings= embed_wiki_content(topic, embedding_model)
        
        # upsert into qdrant
        upsert_embeddings(client, "wiki_chunks", embeddings, chunks, topic)


    # embed query
    query_vector = embed_text([query], embedding_model)[0]

    # search Qdrant
    results = search_qdrant(client, "wiki_chunks", query_vector, topic, limit=limit)

    # run LLM
    answer = run_llm(llm, results, query)

    return answer, results

# testing
# query= "Why is Artificial Intelligence useful?"

# model= get_model()
# query_vector= embed_text([query], model)[0]

# client= ConnectToQuadrant()

# results= search_qdrant(client, "wiki_chunks", query_vector, limit=5)

# mistral= load_mistral(r"C:\Users\thesm\Documents\Personal Website\RAG Project\models\mistral-7b-instruct-v0.1.Q4_K_M.gguf", n_gpu_layers=20, n_ctx=2048)

# answer= run_llm(mistral, results, query)

# print("Context:\n", results)

# print("Answer:\n", answer)

